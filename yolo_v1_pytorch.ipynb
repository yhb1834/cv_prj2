{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_v1_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2ifqJRboccn9xBiW0Kojq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhb1834/cv_prj2/blob/main/yolo_v1_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict\n",
        "!pip install -U albumentations\n",
        "!pip install \"opencv-python-headless<4.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8gbAP2yssFj",
        "outputId": "9b920eb3-9091-4949-c9c0-6256bba90257"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.3 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (4.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.6.0.66 qudida-0.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.6.0.66\n",
            "    Uninstalling opencv-python-headless-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-headless-4.6.0.66\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eL7XTAuNsglc"
      },
      "outputs": [],
      "source": [
        "import torch # 파이토치\n",
        "import torch.nn as nn # 레이어, 활성화함수 등 basic building blocks이 들어있습니다.\n",
        "import torchvision.transforms as transforms # 변환 연산(resize 등)이 들어있는 모듈입니다. \n",
        "from torchvision.datasets import VOCDetection # PASCAL VOC 2007을 가져오는데 쓰입니다. 클래스 형식으로 만들어져 사용을 편리하게 할 수 있습니다.\n",
        "\n",
        "import xmltodict # xml파일의 내용을 딕셔너리에 저장할 수 있는 메소드들이 들어있는 모듈입니다. \n",
        "from PIL import Image # 이미지를 읽기 위해 사용합니다\n",
        "import numpy as np # 넘파이, 저는 넘파이로 연산하는게 익숙해 넘파이를 사용합니다.\n",
        "from tqdm import tqdm # tqdm, for문의 진행상황을 보기 위해 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLO_PASCAL_VOC(VOCDetection):\n",
        "    def __getitem__(self, index):\n",
        "        img = (Image.open(self.images[index]).convert('RGB')).resize((224, 224))\n",
        "        img_transform = transforms.Compose([transforms.PILToTensor(), transforms.Resize((224, 224))])\n",
        "        img = torch.divide(img_transform(img), 255)\n",
        "\n",
        "\n",
        "        target = xmltodict.parse(open(self.annotations[index]).read())\n",
        "\n",
        "        classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
        "                   \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "                   \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
        "                   \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "        label = np.zeros((7, 7, 25), dtype = float)\n",
        "\n",
        "        Image_Height = float(target['annotation']['size']['height'])\n",
        "        Image_Width  = float(target['annotation']['size']['width'])\n",
        "\n",
        "        # 바운딩 박스 정보 받아오기\n",
        "        try:\n",
        "            for obj in target['annotation']['object']:\n",
        "                \n",
        "                # class의 index 휙득\n",
        "                class_index = classes.index(obj['name'].lower())\n",
        "                \n",
        "                # min, max좌표 얻기\n",
        "                x_min = float(obj['bndbox']['xmin']) \n",
        "                y_min = float(obj['bndbox']['ymin'])\n",
        "                x_max = float(obj['bndbox']['xmax']) \n",
        "                y_max = float(obj['bndbox']['ymax'])\n",
        "\n",
        "                # 224*224에 맞게 변형시켜줌\n",
        "                x_min = float((224.0/Image_Width)*x_min)\n",
        "                y_min = float((224.0/Image_Height)*y_min)\n",
        "                x_max = float((224.0/Image_Width)*x_max)\n",
        "                y_max = float((224.0/Image_Height)*y_max)\n",
        "\n",
        "                # 변형시킨걸 x,y,w,h로 만들기 \n",
        "                x = (x_min + x_max)/2.0\n",
        "                y = (y_min + y_max)/2.0\n",
        "                w = x_max - x_min\n",
        "                h = y_max - y_min\n",
        "\n",
        "                # x,y가 속한 cell알아내기\n",
        "                x_cell = int(x/32) # 0~6\n",
        "                y_cell = int(y/32) # 0~6\n",
        "                # cell의 중심 좌표는 (0.5, 0.5)다\n",
        "                x_val_inCell = float((x - x_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
        "                y_val_inCell = float((y - y_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
        "\n",
        "                # w, h 를 0~1 사이의 값으로 만들기\n",
        "                w = w / 224.0\n",
        "                h = h / 224.0\n",
        "\n",
        "                class_index_inCell = class_index + 5\n",
        "\n",
        "                label[y_cell][x_cell][0] = x_val_inCell\n",
        "                label[y_cell][x_cell][1] = y_val_inCell\n",
        "                label[y_cell][x_cell][2] = w\n",
        "                label[y_cell][x_cell][3] = h\n",
        "                label[y_cell][x_cell][4] = 1.0\n",
        "                label[y_cell][x_cell][class_index_inCell] = 1.0\n",
        "\n",
        "\n",
        "        # single-object in image\n",
        "        except TypeError as e : \n",
        "            # class의 index 휙득\n",
        "            class_index = classes.index(target['annotation']['object']['name'].lower())\n",
        "                \n",
        "            # min, max좌표 얻기\n",
        "            x_min = float(target['annotation']['object']['bndbox']['xmin']) \n",
        "            y_min = float(target['annotation']['object']['bndbox']['ymin'])\n",
        "            x_max = float(target['annotation']['object']['bndbox']['xmax']) \n",
        "            y_max = float(target['annotation']['object']['bndbox']['ymax'])\n",
        "\n",
        "            # 224*224에 맞게 변형시켜줌\n",
        "            x_min = float((224.0/Image_Width)*x_min)\n",
        "            y_min = float((224.0/Image_Height)*y_min)\n",
        "            x_max = float((224.0/Image_Width)*x_max)\n",
        "            y_max = float((224.0/Image_Height)*y_max)\n",
        "\n",
        "            # 변형시킨걸 x,y,w,h로 만들기 \n",
        "            x = (x_min + x_max)/2.0\n",
        "            y = (y_min + y_max)/2.0\n",
        "            w = x_max - x_min\n",
        "            h = y_max - y_min\n",
        "\n",
        "            # x,y가 속한 cell알아내기\n",
        "            x_cell = int(x/32) # 0~6\n",
        "            y_cell = int(y/32) # 0~6\n",
        "            x_val_inCell = float((x - x_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
        "            y_val_inCell = float((y - y_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
        "\n",
        "            # w, h 를 0~1 사이의 값으로 만들기\n",
        "            w = w / 224.0\n",
        "            h = h / 224.0\n",
        "\n",
        "            class_index_inCell = class_index + 5\n",
        "\n",
        "            label[y_cell][x_cell][0] = x_val_inCell\n",
        "            label[y_cell][x_cell][1] = y_val_inCell\n",
        "            label[y_cell][x_cell][2] = w\n",
        "            label[y_cell][x_cell][3] = h\n",
        "            label[y_cell][x_cell][4] = 1.0\n",
        "            label[y_cell][x_cell][class_index_inCell] = 1.0\n",
        "            \n",
        "        return img, torch.tensor(label)"
      ],
      "metadata": {
        "id": "zO_XR-YFsi0R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ouohVhTlt2Sr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPc28athngvMDlKDv6mdIdK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db5ab6745e9a485d81f8612473f671de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b968630d97d48b5a60397de4b4c4630",
              "IPY_MODEL_0c22ef1877dd4c6d81e4bf48e3c3c757",
              "IPY_MODEL_d7bfad6e2fe64225b28488b84b4d6cae"
            ],
            "layout": "IPY_MODEL_66461bfdbd4e497eaac63f44428fd9a6"
          }
        },
        "3b968630d97d48b5a60397de4b4c4630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827e46f18072442b9a1549e630aea75d",
            "placeholder": "​",
            "style": "IPY_MODEL_227849b0a7f24a1fa72b4bdc830bb36c",
            "value": "  0%"
          }
        },
        "0c22ef1877dd4c6d81e4bf48e3c3c757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45053a7bb186409ebc11c6362a20ab09",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7810d2260af54cc18ec020176de2a37d",
            "value": 0
          }
        },
        "d7bfad6e2fe64225b28488b84b4d6cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dabc7d8687c94572a7ae80a3922f7bc2",
            "placeholder": "​",
            "style": "IPY_MODEL_6681f1dbee1247bbbb79effb36d66594",
            "value": " 0/48 [00:00&lt;?, ?it/s]"
          }
        },
        "66461bfdbd4e497eaac63f44428fd9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827e46f18072442b9a1549e630aea75d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227849b0a7f24a1fa72b4bdc830bb36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45053a7bb186409ebc11c6362a20ab09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7810d2260af54cc18ec020176de2a37d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dabc7d8687c94572a7ae80a3922f7bc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6681f1dbee1247bbbb79effb36d66594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhb1834/cv_prj2/blob/main/yolov1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download VOC Dataset\n",
        "\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P train/\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P test/\n",
        "!tar -xf test/VOCtest_06-Nov-2007.tar -C test/\n",
        "!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/\n",
        "!rm -rf test/VOCtest_06-Nov-2007.tar"
      ],
      "metadata": {
        "id": "lhYxg45t0B2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download VOC Dataset\n",
        "\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P train/\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P test/\n",
        "!tar -xf test/VOCtest_06-Nov-2007.tar -C test/\n",
        "!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/\n",
        "!rm -rf test/VOCtest_06-Nov-2007.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B149SS1-0gE8",
        "outputId": "545046dd-2fa9-4427-f14c-b55fef20f32c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-07 07:37:06--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n",
            "--2022-06-07 07:37:07--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460032000 (439M) [application/octet-stream]\n",
            "Saving to: ‘train/VOCtrainval_06-Nov-2007.tar’\n",
            "\n",
            "VOCtrainval_06-Nov- 100%[===================>] 438.72M  17.6MB/s    in 26s     \n",
            "\n",
            "2022-06-07 07:37:33 (17.0 MB/s) - ‘train/VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2022-06-07 07:37:33--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 451020800 (430M) [application/octet-stream]\n",
            "Saving to: ‘test/VOCtest_06-Nov-2007.tar’\n",
            "\n",
            "VOCtest_06-Nov-2007 100%[===================>] 430.13M  9.49MB/s    in 47s     \n",
            "\n",
            "2022-06-07 07:38:21 (9.18 MB/s) - ‘test/VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Package\n",
        "\n",
        "!pip install xmltodict\n",
        "!pip install -U albumentations\n",
        "!pip install \"opencv-python-headless<4.3\""
      ],
      "metadata": {
        "id": "zDfStU140nLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Package\n",
        "\n",
        "!pip install xmltodict\n",
        "!pip install -U albumentations\n",
        "!pip install \"opencv-python-headless<4.3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xqexfbxGmnV",
        "outputId": "2851d629-eee9-4103-cfca-d01d44c09132"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 29.4 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (4.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.5.64 qudida-0.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.6)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.64\n",
            "    Uninstalling opencv-python-headless-4.5.5.64:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.64\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configs\n",
        "root_dir = '/content'\n",
        "annot_f = './{}/VOCdevkit/VOC2007/Annotations'\n",
        "image_f = './{}/VOCdevkit/VOC2007/JPEGImages/{}'\n",
        "\n",
        "classes = ['person', # Person\n",
        "           'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', # Animal\n",
        "           'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', # Vehicle\n",
        "           'bottle', 'chair', 'dining table', 'potted plant', 'sofa', 'tv/monitor' # Indoor\n",
        "           ]\n",
        "\n",
        "num_classes = len(classes)\n",
        "feature_size = 7\n",
        "num_bboxes = 2"
      ],
      "metadata": {
        "id": "Qvi5oloUzb3u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VPMq5UVWGfb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(root_dir)\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## utils\n",
        "import numpy as np\n",
        "import random, math, time\n",
        "# 노트북일 때만 사용\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## File Loader\n",
        "import os, xmltodict\n",
        "import os.path as pth\n",
        "from PIL import Image\n",
        "\n",
        "# Draw Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "## Transformer\n",
        "from albumentations.pytorch import transforms\n",
        "from random import sample\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "# Seed\n",
        "random.seed(53)"
      ],
      "metadata": {
        "id": "nCfRWf84zEcs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JBCEmtSSGeOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "def draw_image(image_info, w=448, h=448, transforms=None):\n",
        "    im = np.array(Image.open(image_f.format('train', image_info['image_id'])).convert('RGB').resize((w,h)), dtype=np.uint8)\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig,ax = plt.subplots(1, figsize=(7,7))\n",
        "\n",
        "    bb = image_info['bboxs']\n",
        "    la = image_info['labels']\n",
        "\n",
        "    if transforms:\n",
        "        sample = transforms(image=im, bboxes=bb, category_ids=la)\n",
        "        im = sample['image'].permute(1,2,0).numpy()\n",
        "        bb = sample['bboxes']\n",
        "        la = sample['category_ids']\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "\n",
        "    # Create a Rectangle patch\n",
        "    for b, l in zip(bb, la):\n",
        "        # top left (x, y) , (w, h)\n",
        "        rect = patches.Rectangle((b[0]*w,b[1]*h),(b[2]-b[0])*w,(b[3]-b[1])*h,linewidth=1,edgecolor='r',facecolor='none')\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        props = dict(boxstyle='round', facecolor='red', alpha=0.9)\n",
        "        plt.text(b[0]*w, b[1]*h, classes[l], fontsize=10, color='white', bbox=props)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "L6XKaA9P0xgu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset # VOC\n",
        "def get_infos(annot_f=annot_f, mode='train'):\n",
        "    annot_dir = annot_f.format(mode)\n",
        "    result = []\n",
        "    for ano in [pth.join(annot_dir, ano) for ano in os.listdir(annot_dir)]:\n",
        "        f = open(ano)\n",
        "        info = xmltodict.parse(f.read())['annotation']\n",
        "        image_id = info['filename']\n",
        "        image_size = np.asarray(tuple(map(int, info['size'].values()))[:2], np.int16)\n",
        "        w, h = image_size\n",
        "        box_objects = info['object']\n",
        "        labels = []\n",
        "        bboxs = []\n",
        "        for obj in box_objects:\n",
        "            try:\n",
        "                labels.append(classes.index(obj['name'].lower()))\n",
        "                bboxs.append(tuple(map(int, obj['bndbox'].values())))\n",
        "            except: pass\n",
        "\n",
        "        # Resizing Box, Change x1 y1 x2 y2\n",
        "        # albumentations (normalized box)\n",
        "        bboxs = np.asarray(bboxs, dtype=np.float64)\n",
        "        try:\n",
        "            bboxs[:, [0,2]] /= w\n",
        "            bboxs[:, [1,3]] /= h\n",
        "        except: pass\n",
        "        if bboxs.shape[0] or mode=='test':\n",
        "            result.append({'image_id':image_id, 'image_size':image_size, 'bboxs':bboxs, 'labels':labels})\n",
        "\n",
        "    return result\n",
        "    \n",
        "trval_list = get_infos()\n",
        "test_list = get_infos(mode='test')\n",
        "\n",
        "len(trval_list), len(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfNY8wyl1gmV",
        "outputId": "30525f47-34b0-4f14-c519-44d7848361b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3067, 4952)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "def get_tv_idx(tl, k = 0.5):\n",
        "    total_idx = range(tl)\n",
        "    train_idx = sample(total_idx, int(tl*k))\n",
        "    valid_idx = set(total_idx) - set(train_idx)\n",
        "    return train_idx, list(valid_idx)\n",
        "\n",
        "train_idx, valid_idx = get_tv_idx(len(trval_list))\n",
        "\n",
        "trval_list = np.asarray(trval_list)\n",
        "train_list = trval_list[train_idx]\n",
        "valid_list = trval_list[valid_idx]"
      ],
      "metadata": {
        "id": "nVZfS9d81jMx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Dataset\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, data_list, mode='train', transforms=None):\n",
        "        self.data_list = data_list\n",
        "        self.mode = mode\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        record = self.data_list[idx]\n",
        "        img_id = record['image_id']\n",
        "        bboxs = record['bboxs']\n",
        "        labels = record['labels']\n",
        "\n",
        "        img = Image.open(image_f.format(self.mode, img_id)).convert('RGB') #.resize((800,800))\n",
        "        img = np.array(img)\n",
        "\n",
        "        if self.transforms:\n",
        "            for t in self.transforms:\n",
        "                sample = self.transforms(image=img, bboxes=bboxs, category_ids=labels)\n",
        "                image = sample['image']\n",
        "                bboxs = np.asarray(sample['bboxes'])\n",
        "                labels = np.asarray(sample['category_ids'])\n",
        "\n",
        "\n",
        "        if self.mode=='train':\n",
        "            target = encode(bboxs, labels)\n",
        "            return image, target\n",
        "        else:\n",
        "            return image"
      ],
      "metadata": {
        "id": "XfTRt7VR14m5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "def encode(bboxs, labels):\n",
        "    # Make YoLo Target\n",
        "\n",
        "    S = feature_size\n",
        "    B = num_bboxes\n",
        "    N = 5 * B + num_classes\n",
        "    cell_size = 1.0 / float(S)\n",
        "    # print(bboxs.shape)\n",
        "\n",
        "    box_cxy = (bboxs[:, 2:] + bboxs[:, :2])/2.0\n",
        "    box_wh = bboxs[:, 2:] - bboxs[:, :2]\n",
        "    target = np.zeros((S, S, N))\n",
        "    for b in range(bboxs.shape[0]):\n",
        "        cxy, wh, label = box_cxy[b], box_wh[b], labels[b]\n",
        "        ij = np.ceil(cxy / cell_size) - 1.0\n",
        "        i, j = map(int, ij)\n",
        "        top_left = ij*cell_size\n",
        "        dxy_norm = (cxy-top_left)/cell_size\n",
        "        for k in range(B):\n",
        "            target[j, i, 5*k: 5*(k+1)] = np.r_[dxy_norm, wh, 1]\n",
        "        target[j, i, 5*B+label] = 1.0\n",
        "    return target"
      ],
      "metadata": {
        "id": "N3NA4Uie_qdj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer 재정의\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(448,448, always_apply=True, p=1),\n",
        "        # A.Cutout(num_holes=7, max_h_size=16, max_w_size=16, fill_value=0, always_apply=False, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.HorizontalFlip(),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(format='albumentations', label_fields=['category_ids']))\n",
        "\n",
        "def get_test_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(448,448, always_apply=True, p=1),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "n2ZCBo1q_vfO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = VOCDataset(train_list, transforms=get_train_transforms())\n",
        "valid_ds = VOCDataset(valid_list, transforms=get_test_transforms())\n",
        "test_ds = VOCDataset(test_list, mode='test', transforms=get_test_transforms())\n",
        "\n",
        "# torch tensor를 batch size만큼 묶어줌\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0), torch.FloatTensor(targets)\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    images = batch\n",
        "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=test_collate_fn)"
      ],
      "metadata": {
        "id": "K4WqNknF_yS4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YoLo\n",
        "class YoLo_v1(nn.Module):\n",
        "    def __init__(self, num_classes=20, num_bboxes=2):\n",
        "        super(YoLo_v1, self).__init__()\n",
        "\n",
        "        self.feature_size = 7\n",
        "        self.num_bboxes=num_bboxes\n",
        "        self.num_classes=num_classes\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=4),\n",
        "            # nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(192),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=192, out_channels=128, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0),\n",
        "            # nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=2, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "            # nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(in_features=7*7*1024, out_features=4096),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=(feature_size*feature_size*(5 * num_bboxes + num_classes))),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "            \n",
        "        self.init_weight(self.conv)\n",
        "        self.init_weight(self.fc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        s, b, c = self.feature_size, self.num_bboxes, self.num_classes\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        x = x.view(-1, s, s, (5 * b + c))\n",
        "        return x\n",
        "\n",
        "    def init_weight(self, modules):\n",
        "        for m in modules:\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class Squeeze(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Squeeze, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.squeeze()\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "70sL13y5AuKt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(bbox1, bbox2):\n",
        "    \"\"\" Compute the IoU (Intersection over Union) of two set of bboxes, each bbox format: [x1, y1, x2, y2].\n",
        "    Args:\n",
        "        bbox1: (Tensor) bounding bboxes, sized [N, 4].\n",
        "        bbox2: (Tensor) bounding bboxes, sized [M, 4].\n",
        "    Returns:\n",
        "        (Tensor) IoU, sized [N, M].\n",
        "    \"\"\"\n",
        "    N = bbox1.size(0)\n",
        "    M = bbox2.size(0)\n",
        "\n",
        "    # Compute left-top coordinate of the intersections\n",
        "    lt = torch.max(\n",
        "        bbox1[:, :2].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
        "        bbox2[:, :2].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
        "    )\n",
        "    # Conpute right-bottom coordinate of the intersections\n",
        "    rb = torch.min(\n",
        "        bbox1[:, 2:].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
        "        bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
        "    )\n",
        "    # Compute area of the intersections from the coordinates\n",
        "    wh = rb - lt   # width and height of the intersection, [N, M, 2]\n",
        "    wh[wh < 0] = 0 # clip at 0\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1] # [N, M]\n",
        "\n",
        "    # Compute area of the bboxes\n",
        "    area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1]) # [N, ]\n",
        "    area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1]) # [M, ]\n",
        "    area1 = area1.unsqueeze(1).expand_as(inter) # [N, ] -> [N, 1] -> [N, M]\n",
        "    area2 = area2.unsqueeze(0).expand_as(inter) # [M, ] -> [1, M] -> [N, M]\n",
        "\n",
        "    # Compute IoU from the areas\n",
        "    union = area1 + area2 - inter # [N, M, 2]\n",
        "    iou = inter / union           # [N, M, 2]\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "r4CS1JiwDqNb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(pred_tensor, target_tensor):\n",
        "    \"\"\" Compute loss for YOLO training.\n",
        "    Args:\n",
        "        pred_tensor: (Tensor) predictions, sized [n_batch, S, S, Bx5+C], 5=len([x, y, w, h, conf]).\n",
        "        target_tensor: (Tensor) targets, sized [n_batch, S, S, Bx5+C].\n",
        "    Returns:\n",
        "        (Tensor): loss, sized [1, ].\n",
        "    \"\"\"\n",
        "    # TODO: Romove redundant dimensions for some Tensors.\n",
        "\n",
        "    S = feature_size\n",
        "    B = num_bboxes\n",
        "    C = num_classes\n",
        "    N = 5 * B + C \n",
        "    lambda_coord = 5#torch.tensor().cuda()\n",
        "    lambda_noobj = 0.5#torch.tensor(0.5).cuda()\n",
        "\n",
        "\n",
        "    batch_size = pred_tensor.size(0)\n",
        "    coord_mask = target_tensor[:, :, :, 4] > 0  # mask for the cells which contain objects. [n_batch, S, S]\n",
        "    noobj_mask = target_tensor[:, :, :, 4] == 0 # mask for the cells which do not contain objects. [n_batch, S, S]\n",
        "    coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
        "    noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
        "\n",
        "    coord_pred = pred_tensor[coord_mask].view(-1, N)            # pred tensor on the cells which contain objects. [n_coord, N]\n",
        "                                                                # n_coord: number of the cells which contain objects.\n",
        "    bbox_pred = coord_pred[:, :5*B].contiguous().view(-1, 5)    # [n_coord x B, 5=len([x, y, w, h, conf])]\n",
        "    class_pred = coord_pred[:, 5*B:]                            # [n_coord, C]\n",
        "\n",
        "    coord_target = target_tensor[coord_mask].view(-1, N)        # target tensor on the cells which contain objects. [n_coord, N]\n",
        "                                                                # n_coord: number of the cells which contain objects.\n",
        "    bbox_target = coord_target[:, :5*B].contiguous().view(-1, 5)# [n_coord x B, 5=len([x, y, w, h, conf])]\n",
        "    class_target = coord_target[:, 5*B:]                        # [n_coord, C]\n",
        "\n",
        "    # Compute loss for the cells with no object bbox.\n",
        "    noobj_pred = pred_tensor[noobj_mask].view(-1, N)        # pred tensor on the cells which do not contain objects. [n_noobj, N]\n",
        "                                                            # n_noobj: number of the cells which do not contain objects.\n",
        "    noobj_target = target_tensor[noobj_mask].view(-1, N)    # target tensor on the cells which do not contain objects. [n_noobj, N]\n",
        "                                                            # n_noobj: number of the cells which do not contain objects.\n",
        "    noobj_conf_mask = torch.cuda.ByteTensor(noobj_pred.size()).fill_(0) # [n_noobj, N]\n",
        "    for b in range(B):\n",
        "        noobj_conf_mask[:, 4 + b*5] = 1 # noobj_conf_mask[:, 4] = 1; noobj_conf_mask[:, 9] = 1\n",
        "    noobj_pred_conf = noobj_pred[noobj_conf_mask]       # [n_noobj, 2=len([conf1, conf2])]\n",
        "    noobj_target_conf = noobj_target[noobj_conf_mask]   # [n_noobj, 2=len([conf1, conf2])]\n",
        "    loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\n",
        "\n",
        "    # Compute loss for the cells with objects.\n",
        "    coord_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(0)    # [n_coord x B, 5]\n",
        "    coord_not_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(1)# [n_coord x B, 5]\n",
        "    bbox_target_iou = torch.zeros(bbox_target.size()).cuda()                    # [n_coord x B, 5], only the last 1=(conf,) is used\n",
        "\n",
        "    # Choose the predicted bbox having the highest IoU for each target bbox.\n",
        "    for i in range(0, bbox_target.size(0), B):\n",
        "        pred = bbox_pred[i:i+B] # predicted bboxes at i-th cell, [B, 5=len([x, y, w, h, conf])]\n",
        "        pred_xyxy = Variable(torch.FloatTensor(pred.size())) # [B, 5=len([x1, y1, x2, y2, conf])]\n",
        "        # Because (center_x,center_y)=pred[:, 2] and (w,h)=pred[:,2:4] are normalized for cell-size and image-size respectively,\n",
        "        # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
        "        pred_xyxy[:,  :2] = pred[:, :2]/float(S) - 0.5 * pred[:, 2:4]\n",
        "        pred_xyxy[:, 2:4] = pred[:, :2]/float(S) + 0.5 * pred[:, 2:4]\n",
        "\n",
        "        target = bbox_target[i] # target bbox at i-th cell. Because target boxes contained by each cell are identical in current implementation, enough to extract the first one.\n",
        "        target = bbox_target[i].view(-1, 5) # target bbox at i-th cell, [1, 5=len([x, y, w, h, conf])]\n",
        "        target_xyxy = Variable(torch.FloatTensor(target.size())) # [1, 5=len([x1, y1, x2, y2, conf])]\n",
        "        # Because (center_x,center_y)=target[:, 2] and (w,h)=target[:,2:4] are normalized for cell-size and image-size respectively,\n",
        "        # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
        "        target_xyxy[:,  :2] = target[:, :2]/float(S) - 0.5 * target[:, 2:4]\n",
        "        target_xyxy[:, 2:4] = target[:, :2]/float(S) + 0.5 * target[:, 2:4]\n",
        "\n",
        "        iou = compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4]) # [B, 1]\n",
        "        max_iou, max_index = iou.max(0)\n",
        "        max_index = max_index.data.cuda()\n",
        "\n",
        "        coord_response_mask[i+max_index] = 1\n",
        "        coord_not_response_mask[i+max_index] = 0\n",
        "\n",
        "        # \"we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth\"\n",
        "        # from the original paper of YOLO.\n",
        "        bbox_target_iou[i+max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
        "    bbox_target_iou = Variable(bbox_target_iou).cuda()\n",
        "\n",
        "    # BBox location/size and objectness loss for the response bboxes.\n",
        "    bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)      # [n_response, 5]\n",
        "    bbox_target_response = bbox_target[coord_response_mask].view(-1, 5)  # [n_response, 5], only the first 4=(x, y, w, h) are used\n",
        "    target_iou = bbox_target_iou[coord_response_mask].view(-1, 5)        # [n_response, 5], only the last 1=(conf,) is used\n",
        "    loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
        "    loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]), torch.sqrt(bbox_target_response[:, 2:4]), reduction='sum')\n",
        "    loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\n",
        "\n",
        "    # Class probability loss for the cells which contain objects.\n",
        "    loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\n",
        "\n",
        "    # Total loss\n",
        "    loss = lambda_coord * (loss_xy + loss_wh) + loss_obj + lambda_noobj * loss_noobj + loss_class\n",
        "    loss = loss / float(batch_size)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "9xU5LLu7Fa4t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = YoLo_v1().cuda()\n",
        "# criterion = loss_fn().cuda()\n",
        "init_lr = 0.001\n",
        "base_lr = 0.01\n",
        "optimizer = optim.SGD(yolo.parameters(), lr=init_lr, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "def update_lr(optimizer, epoch, step_per, burnin_exp=4.0):\n",
        "    if epoch in range(50):\n",
        "        lr = init_lr + (base_lr - init_lr) * math.pow(epoch/(50-1), burnin_exp)\n",
        "    elif epoch == 50:\n",
        "        lr = base_lr\n",
        "    elif epoch == 750:\n",
        "        lr = 0.001\n",
        "    elif epoch == 1050:\n",
        "        lr = 0.0001\n",
        "    else: return\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "coyfMwygFgZV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "bl = len(train_loader)\n",
        "history = {'total_loss':[]}\n",
        "for epoch in range(1500):  # loop over the dataset multiple times\n",
        "    tk0 = tqdm(train_loader, total=bl,leave=False)\n",
        "    t_loss = 0.0\n",
        "    breaking=False\n",
        "    for step, (image, target) in enumerate(tk0, 0):\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        update_lr(optimizer, epoch, float(step) / float(bl - 1))\n",
        "        output = yolo(image)\n",
        "        loss = loss_fn(output, target).cuda()\n",
        "\n",
        "        if math.isnan(loss):\n",
        "            print(loss)\n",
        "            breaking = True\n",
        "            break\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        t_loss += loss.item()\n",
        "\n",
        "        history['total_loss'].append(loss.item())\n",
        "\n",
        "    if breaking:\n",
        "        break\n",
        "\n",
        "    # print statistics\n",
        "    tqdm.write('[Epoch : %d] total_loss: %.5f Total_elapsed_time: %d 분' %\n",
        "        (epoch + 1, t_loss / bl, (time.time()-start_time)/60))\n",
        "\n",
        "print(time.time()-start_time)\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "db5ab6745e9a485d81f8612473f671de",
            "3b968630d97d48b5a60397de4b4c4630",
            "0c22ef1877dd4c6d81e4bf48e3c3c757",
            "d7bfad6e2fe64225b28488b84b4d6cae",
            "66461bfdbd4e497eaac63f44428fd9a6",
            "827e46f18072442b9a1549e630aea75d",
            "227849b0a7f24a1fa72b4bdc830bb36c",
            "45053a7bb186409ebc11c6362a20ab09",
            "7810d2260af54cc18ec020176de2a37d",
            "dabc7d8687c94572a7ae80a3922f7bc2",
            "6681f1dbee1247bbbb79effb36d66594"
          ]
        },
        "id": "La_EqSe9FkBK",
        "outputId": "a5c7d489-2ffa-4a75-ddd4-a8491f97e4a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/48 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db5ab6745e9a485d81f8612473f671de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2828de51a4de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mupdate_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-773dd7eaa65d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qQ-odkfuHGre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}